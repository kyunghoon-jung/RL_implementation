{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë¶„ì‚° ê°•í™”í•™ìŠµì„ DQNì„ ì´ìš©í•˜ì—¬ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤. <br>ê¸°ë³¸ì ì¸ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. <br>  \n",
    "    1. Replay Buffer: Actorë¡œë¶€í„° dataë¥¼ ë°›ê³ , Learnerì—ê²Œ dataë¥¼ ì „ë‹¬í•˜ëŠ” ì—­í• \n",
    "    2. Parameter Server: Learnerë¡œë¶€í„° parameterë¥¼ ë°›ê³ , Actorì—ê²Œ paramterë¥¼ ì „ë‹¬í•˜ëŠ” ì—­í• .\n",
    "    3. Learner: Replay Bufferë¡œ ë¶€í„° ë°ì´í„°ë¥¼ ë°›ì•„ í•™ìŠµì„ ì§„í–‰í•˜ê³ , Parameter Serverë¡œ Learner ëª¨ë¸ì˜ parameterë¥¼ ì „ë‹¬í•˜ëŠ” ì—­í• .\n",
    "    4. Actor: Environmentì™€ ìƒí˜¸ì‘ìš©í•˜ë©° dataë¥¼ Replay Bufferì— ì „ë‹¬í•˜ê³ , Parameter Serverë¡œë¶€í„° Learner ëª¨ë¸ì˜ parameterë¥¼ ë°›ì•„ ìì‹ ì˜ ëª¨ë¸ parameterë¥¼ update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5ë²ˆ ë…¸íŠ¸ë¶ê³¼ëŠ” ë‹¬ë¦¬ \"with restricted update steps\" ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ëºìŠµë‹ˆë‹¤.<br>\n",
    "\n",
    "    ì´ë²ˆ DQNì€ actorì™€ learnerê°€ ëª¨ë¸ì˜ parameterë¥¼ ì£¼ê³  ë°›ëŠ” íƒ€ì´ë°, actorê°€ environmentì™€ ìƒí˜¸ì‘ìš©í•˜ë©° stepì„ ì§„í–‰í•˜ëŠ” ì†ë„ ë“±ì„ \n",
    "    ì¸ìœ„ì ìœ¼ë¡œ ì¡°ì ˆí•˜ì§€ ì•Šì€ ëª¨ë¸ì…ë‹ˆë‹¤. \n",
    "    \n",
    "    5ë²ˆ ì´ì™¸ì— ì´ ë…¸íŠ¸ë¶ íŒŒì¼ì„ ë‘” ì´ìœ ëŠ”, ì¸ìœ„ì ì¸ ì¡°ì ˆì—†ì´ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì œê°€(ê·¸ë¦¬ê³  A3Cë‚˜ Ape-Xë“±ì˜ ë…¼ë¬¸ì—ì„œë„) ë³¸ë˜ êµ¬í˜„í•˜ê³ ì í–ˆë˜ ê²ƒì´ê¸° ë•Œë¬¸ì¸ë°ìš”.\n",
    "    \n",
    "    ë‹¤ë§Œ, hyperparameter ì„¤ì •ì„ ì˜ í•´ì£¼ì•¼ actorì™€ learnerì˜ ê· í˜•ì´ ë§ìŠµë‹ˆë‹¤.\n",
    "<br>\n",
    "\n",
    "#### ê·¸ë ‡ë‹¤ë©´ 5ë²ˆê³¼ì˜ ì½”ë“œìƒì˜ ì°¨ì´ì ì€?<br>\n",
    "\n",
    "    Actorì™€ Learnerì˜ ë©”ì†Œë“œì— ë“¤ì–´ìˆëŠ”, while 1: ê³¼ if ray.get(self.memory.return_batch_update_status.remote()) ê°€ ë“¤ì–´ê°€ ìˆëŠ” loop ë“± ëª‡ê°€ì§€ loopê³¼ ifë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. \n",
    "    ì´ì™¸ì—ëŠ” ë™ì¼í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    time.sleep() ìœ¼ë¡œ learnerì™€ actorì˜ ì†ë„ë¥¼ ì¡°ì ˆí•˜ê²Œë” í•˜ì˜€ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install ray\n",
    "    !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray \n",
    "import gym\n",
    "import time \n",
    "import numpy as np \n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 11:27:30,769\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.61',\n",
       " 'raylet_ip_address': '192.168.0.61',\n",
       " 'redis_address': '192.168.0.61:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-02-22_11-27-30_285134_59891/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-02-22_11-27-30_285134_59891/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-02-22_11-27-30_285134_59891',\n",
       " 'metrics_export_port': 53828,\n",
       " 'node_id': '5a64fcb07310245f093e2ffd5f7eddc314e6ef2b'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bufferë¥¼ ì •ì˜í•©ë‹ˆë‹¤. \n",
    "@ray.remote \n",
    "class ReplayBuffer: \n",
    "    def __init__(self, \n",
    "                   buffer_size: ('int: Buffer_size'), \n",
    "                 state_dim: ('tuple: State dim')):\n",
    "\n",
    "        # 1ì°¨ì› stateë¼í• ì§€ë¼ë„ tupleë¡œ ì…ë ¥ë°›ë„ë¡ tuple íƒ€ì…ì„ ê°•ì œí•˜ì˜€ìŠµë‹ˆë‹¤. \n",
    "        # ë°‘ì— ì¤„ì˜ self.buffer_dimì„ êµ¬í•˜ê¸° ìœ„í•´ì„œ ì´ë ‡ê²Œ í•œ ê²ƒì¸ë°ìš”, ì‚¬ì‹¤ ë¹¼ë„ ìƒê´€ì—†ê³  ì–¼ë§ˆë“ ì§€ ë‹¤ë¥´ê²Œ êµ¬í˜„í•˜ì…”ë„ ë¬´ë°©í•©ë‹ˆë‹¤.\n",
    "        # ì°¸ê³ ) rayë¥¼ ì“¸ ë•ŒëŠ”, classì„ ì–¸ ì‹œì— assertì¡°ê±´ì„ ë§Œì¡± ëª»í•˜ì—¬ë„ ì—ëŸ¬ë¥¼ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤! classì˜ ë©”ì†Œë“œë¥¼ ì‹¤í–‰í•˜ê³  ë‚˜ì„œì•¼ __init__ì—ì„œ assertion ì—ëŸ¬ê°€ ìˆë‹¤ê³  í‘œì‹œë¥¼ í•´ì¤ë‹ˆë‹¤. \n",
    "        assert type(state_dim) == tuple\n",
    "        \n",
    "        self.buffer_dim = (buffer_size, ) + state_dim\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_update_status = True\n",
    "        \n",
    "        self.state_buffer = np.zeros(self.buffer_dim)\n",
    "        self.action_buffer = np.zeros(buffer_size)\n",
    "        self.reward_buffer = np.zeros(buffer_size)\n",
    "        self.next_state_buffer = np.zeros(self.buffer_dim)\n",
    "        self.done_buffer = np.zeros(buffer_size)\n",
    "        self.act_idx_buffer = np.zeros(buffer_size)\n",
    "\n",
    "        self.store_idx = 0\n",
    "        self.current_size = 0\n",
    "        self.total_store_count = 0\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done, actor_idx): \n",
    "        self.state_buffer[self.store_idx] = state\n",
    "        self.action_buffer[self.store_idx] = action\n",
    "        self.reward_buffer[self.store_idx] = reward\n",
    "        self.next_state_buffer[self.store_idx] = next_state\n",
    "        self.done_buffer[self.store_idx] = done\n",
    "\n",
    "        # actor_idxëŠ” í•™ìŠµì‹œ ì“°ì´ì§€ ì•Šì§€ë§Œ, ì—¬ëŸ¬ actorë¡œ ë¶€í„° ë°ì´í„° ì €ì¥ì´ ì˜ ë˜ëŠ”ì§€ í™•ì¸ìš© ë³€ìˆ˜ì…ë‹ˆë‹¤\n",
    "        self.act_idx_buffer[self.store_idx] = actor_idx\n",
    "        \n",
    "        self.total_store_count += 1 # í•™ìŠµ ì¤‘ì— ìŒ“ì€ dataì˜ ì´ ê°œìˆ˜ë¥¼ countingí•˜ê¸° ìœ„í•œ ë³€ìˆ˜\n",
    "        self.store_idx = (self.store_idx + 1) % self.buffer_size\n",
    "        self.current_size = min(self.current_size+1, self.buffer_size)\n",
    "    \n",
    "    def batch_load(self, batch_size): \n",
    "        indices = np.random.randint(self.current_size, size=batch_size)  \n",
    "        return dict( \n",
    "                states=self.state_buffer[indices], \n",
    "                actions=self.action_buffer[indices], \n",
    "                rewards=self.reward_buffer[indices], \n",
    "                next_states=self.next_state_buffer[indices], \n",
    "                dones=self.done_buffer[indices],\n",
    "                actindices=self.act_idx_buffer[indices])  \n",
    "    \n",
    "    # ì•„ë˜ì˜ ë©”ì†Œë“œë“¤ì€ rayë¡œ ë‹¤ë¥¸ ê°ì²´ê°€ current_size, store_idx, total_store_count ë³€ìˆ˜ë“¤ì„ ì ‘ê·¼í•  ë•Œ ì“°ê¸° ìœ„í•´ì„œ ì„ ì–¸\n",
    "    def return_current_size(self):\n",
    "        return self.current_size\n",
    "\n",
    "    def return_store_idx(self):\n",
    "        return self.store_idx\n",
    "\n",
    "    def return_total_store_count(self):\n",
    "        return self.total_store_count\n",
    "    \n",
    "    def batch_update_on(self):\n",
    "        self.batch_update_status = True\n",
    "\n",
    "    def batch_update_off(self):\n",
    "        self.batch_update_status = False\n",
    "\n",
    "    def return_batch_update_status(self):\n",
    "        return self.batch_update_status\n",
    "    \n",
    "# # test\n",
    "# buffer_size = 1000\n",
    "# batch_size = 16\n",
    "# state_dim = (4, )\n",
    "# temp_buffer = ReplayBuffer.remote(buffer_size, state_dim)\n",
    "\n",
    "# for i in range(50):\n",
    "#     temp_buffer.store.remote(np.array(state_dim), 1, np.array(state_dim), 1, 1, 1)\n",
    "\n",
    "# batch = temp_buffer.batch_load.remote(batch_size)\n",
    "# print(\"Batch Size:\", ray.get(batch)['actindices'].shape) \n",
    "\n",
    "# current_size = temp_buffer.return_current_size.remote()\n",
    "# print(\"Current Size: \", ray.get(current_size))\n",
    "\n",
    "# return_store_idx = temp_buffer.return_store_idx.remote()\n",
    "# print(\"Store Index: \", ray.get(return_store_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden=32):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        state_size = state_size[0]\n",
    "        self.fc1 = nn.Linear(state_size, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# state_size = (4, ) \n",
    "# action_size = 2 \n",
    "# temp_net = QNetwork(state_size, action_size, 32) \n",
    "# test = torch.randn(size=(4,)) \n",
    "# temp_net(test), temp_net(test).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Network_parameter_server:\n",
    "\n",
    "    def update_parameters(self, learner_params): \n",
    "        self.learner_params = learner_params\n",
    "\n",
    "    def return_parameters(self):\n",
    "        return self.learner_params\n",
    "\n",
    "    def return_saving_status(self):\n",
    "        return self.is_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebookì—ì„œ plotting ìš©ë„ë¡œ ì •ì˜í•œ í•¨ìˆ˜\n",
    "@ray.remote\n",
    "class Plot_inline:  \n",
    "    def __init__(self): \n",
    "        # actorì˜ ê°œìˆ˜ê°€ ì—¬ëŸ¬ê°œ ìˆì„ ë•Œ ë™ì ìœ¼ë¡œ dictionaryì˜ keyë¥¼ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ defalutdictë¥¼ í™œìš©\n",
    "        self.score_dict = defaultdict(list) \n",
    "    \n",
    "    def store_actor_data(self, actor_idx, score): \n",
    "        self.score_dict[actor_idx].append(score) \n",
    "        \n",
    "    def store_learner_data(self, score): \n",
    "        self.score_dict['learner'].append(score) \n",
    "\n",
    "    def get_status(self): \n",
    "        return self.score_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actorì˜ ì—­í• ì€ ê°ê° envì—ì„œ ê²½í—˜í•œ ê²ƒì„ bufferì— ë„˜ê²¨ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "@ray.remote\n",
    "class Actor:  \n",
    "    def __init__(self, \n",
    "                 params_server: (\"class(ray decorated): Network parameter server\"),\n",
    "                 memory: (\"class(ray decorated): Replay Buffer\"),\n",
    "                 env_name: (\"str: Environment name\"), \n",
    "                 actor_idx: (\"int: The index of an actor\"), \n",
    "                 actor_update_freq: (\"int: Frequency of updating actor's network. (unit: steps)\"),\n",
    "                 update_buf_start: (\"int: Update starting buffer size\"), \n",
    "                 epsilon: (\"int: starting epsilon value for e-greedy update\"), \n",
    "                 eps_decay: (\"int: epsilon decay rate\"), \n",
    "                 eps_min: (\"int: minimum epsilon value\"), \n",
    "                 hidden: (\"int: Update frequency of learner's q_behave network\"), \n",
    "                 device: (\"int: Cuda device number\"),\n",
    "                 plot_mode: (\"str: whether to plot in wandb or inline in jupyter or none-plotting(False)\"),\n",
    "                 plot_util: (\"class(ray decorated): Plotting tool for visualizing in jupyter\"),\n",
    "                 num_actors: (\"int: The number of actors\"),\n",
    "                 WANDB_GROUP_NAME: (\"str: Wandb's group name for all actors\"),\n",
    "                 WANDB_CONFIG: (\"str: Wandb configuration dictionary\")\n",
    "                ):\n",
    "\n",
    "        # wandb init config \n",
    "        if plot_mode=='wandb':\n",
    "            entity = 'rl_flip_school_team'  \n",
    "            project_name = 'Distributed_DQN'\n",
    "            wandb.init(\n",
    "                    group=WANDB_GROUP_NAME,\n",
    "                    project=project_name, \n",
    "                    entity=entity,\n",
    "                    config=WANDB_CONFIG,\n",
    "                    name=f'{actor_idx}_Distributed_DQN'\n",
    "                    ) \n",
    "\n",
    "        self.env = gym.make(env_name)\n",
    "        self.params_server = params_server\n",
    "        self.memory = memory   # rayë¥¼ í†µí•´ ê³µìœ í•˜ëŠ” Replaybuffer classì…ë‹ˆë‹¤.\n",
    "        self.actor_idx = actor_idx # ì–´ë–¤ actorì—ì„œ ì˜¨ ë°ì´í„°ì¸ì§€ ë³´ê¸° ìœ„í•œ ë³€ìˆ˜ì…ë‹ˆë‹¤.\n",
    "        self.actor_update_freq = actor_update_freq\n",
    "        self.update_buf_start = update_buf_start\n",
    "        self.plot_mode = plot_mode\n",
    "        self.plot_util = plot_util\n",
    "        self.num_actors = num_actors\n",
    "        self.device = device\n",
    "        self.time_log = 0 # í•™ìŠµ íš¨ìœ¨ì„ ì‹œê°„ìœ¼ë¡œ Plot í•˜ê¸°ìœ„í•´ ë„ì…í•œ ë³€ìˆ˜\n",
    "\n",
    "        # DQN hyperparameters\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "\n",
    "        # Network parameters\n",
    "        self.state_dim = (self.env.observation_space.shape[0], )\n",
    "        try: self.action_dim = self.env.action_space.n # Discrete action\n",
    "        except: self.action_dim = env.action_space.shape[0] # Continous action            \n",
    "        self.q_behave = QNetwork(self.state_dim, self.action_dim, hidden).to(self.device)\n",
    "\n",
    "    def select_action(self, state): \n",
    "        # e-greedyë¡œ actionì„ ì„ íƒ \n",
    "        if np.random.random() < self.epsilon: \n",
    "            return np.zeros(self.action_dim), self.env.action_space.sample() \n",
    "        else: \n",
    "            state = torch.FloatTensor(state).to(self.device).unsqueeze(0) \n",
    "            Qs = self.q_behave(state) \n",
    "            action = Qs.argmax() \n",
    "            return Qs.detach().cpu().numpy(), action.detach().item() \n",
    "\n",
    "    def make_buffer_ready(self):\n",
    "        # ê³µìœ  ReplayBufferì— update_buf_start ê°œìˆ˜ê¹Œì§€ data ì €ì¥ \n",
    "        state = self.env.reset() \n",
    "        while 1:\n",
    "            Qs, action = self.select_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action) \n",
    "\n",
    "            self.memory.store.remote(state, action, next_state, reward, done, self.actor_idx) \n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = self.env.reset() \n",
    "                if ray.get(self.memory.return_total_store_count.remote()) > self.update_buf_start: break\n",
    "\n",
    "        done_signal = f\"Done in Actor-#{self.actor_idx}\" \n",
    "        return done_signal\n",
    "\n",
    "    def explore(self):\n",
    "        score = 0 \n",
    "        update_freq = 0 \n",
    "        state = self.env.reset() \n",
    "\n",
    "        start_time = int(time.time())  \n",
    "\n",
    "        # actorëŠ” ë©ˆì¶”ì§€ ì•Šê³  ë¬´í•œ loopë¡œ explorationí•˜ë„ë¡ ì„¤ì • \n",
    "        while 1:\n",
    "            Qs, action = self.select_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action) \n",
    "\n",
    "            self.memory.store.remote(state, action, next_state, reward, done, self.actor_idx) # ê³µìœ  ReplayBufferì— ì €ì¥\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            self.epsilon = max(self.epsilon-self.eps_decay, self.eps_min)\n",
    "\n",
    "            update_freq = (update_freq + 1) % self.actor_update_freq\n",
    "#             time.sleep(0.005)\n",
    "            if update_freq==0: self._pull_parameters() \n",
    "\n",
    "            if done:\n",
    "                state = self.env.reset() \n",
    "                self.time_log = int(time.time()) - start_time  # Accumulated Trainning Time (unit: second)\n",
    "                self._plot_status(score)\n",
    "                self.plot_util.store_actor_data.remote(self.actor_idx, score) \n",
    "                score = 0\n",
    "\n",
    "    def _pull_parameters(self):\n",
    "        updated_params = ray.get(self.params_server.return_parameters.remote()) \n",
    "        self.q_behave.load_state_dict(updated_params) \n",
    "\n",
    "    def _plot_status(self, score): \n",
    "        if self.plot_mode=='wandb': \n",
    "            wandb.log({'Score': score, \n",
    "                       f\"Epsilon_{self.actor_idx}\": self.epsilon,\n",
    "                       f'Score_{self.actor_idx}': score}, step=self.time_log) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LearnerëŠ” bufferì— ìˆëŠ” ìƒ˜í”Œì„ ì´ìš©í•˜ì—¬ network parameterë¥¼ ì—…ë°ì´íŠ¸ë¥¼ í•˜ë©°, parameter serverì— network weightì„ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "# LearnerëŠ” network update ë“± cuda ì—°ì‚°ì„ í•˜ê³  cpuë¡œ ë³‘ë ¬ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì—†ìœ¼ë¯€ë¡œ rayë¥¼ ì´ìš©í•˜ì—¬ ì„ ì–¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤(ì •í™•íˆëŠ”, ì„ ì–¸í•  ìˆ˜ ì—†ê²Œ ë˜ì–´ìˆìŠµë‹ˆë‹¤).\n",
    "class Learner: \n",
    "    def __init__(self, \n",
    "                 env_name: (\"str: Environment name\"),\n",
    "                 params_server: (\"Class: Network parameter server\"),\n",
    "                 memory: (\"class: ReplayBuffer\"),\n",
    "                 gamma: (\"float: Discount rate\"), \n",
    "                 epsilon: (\"int: starting epsilon value for e-greedy update\"), \n",
    "                 eps_decay: (\"int: epsilon decay rate\"), \n",
    "                 eps_min: (\"int: minimum epsilon value\"), \n",
    "                 update_freq: (\"int: Frequency of updating learner's q_behave network\"), \n",
    "                 update_target_freq: (\"int: Frequency of updating learner's q_target network\"), \n",
    "                 update_push_freq: (\"int: Frequency of sending learner's paratemers to parameter-server\"), \n",
    "                 hidden: (\"int: Update frequency of learner's q_behave network\"), \n",
    "                 batch_size: (\"int: Batch size for updating network\"),\n",
    "                 learning_rate: (\"float: Learning rate for updating the q_behave network\"),\n",
    "                 device: (\"int: Cuda device number\"),\n",
    "                 plot_mode: (\"str: whether to plot in wandb or inline in jupyter\"),\n",
    "                 plot_util: (\"class(ray decorated): Plotting tool for visualizing in jupyter\"),\n",
    "                 WANDB_GROUP_NAME: (\"str: Wandb's group name for all actors\"),\n",
    "                 WANDB_CONFIG: (\"str: Wandb configuration dictionary\")\n",
    "                ):\n",
    "\n",
    "        if plot_mode=='wandb':\n",
    "            entity = 'rl_flip_school_team'  \n",
    "            project_name = 'Distributed_DQN'\n",
    "            wandb.init(\n",
    "                    group=WANDB_GROUP_NAME,\n",
    "                    project=project_name, \n",
    "                    entity=entity,\n",
    "                    config=WANDB_CONFIG,\n",
    "                    name='Learner_Distributed_DQN'\n",
    "                    ) \n",
    "\n",
    "        self.env = gym.make(env_name)\n",
    "        self.params_server = params_server\n",
    "        self.memory = memory\n",
    "        self.gamma = gamma\n",
    "        self.plot_mode = plot_mode\n",
    "        self.plot_util = plot_util\n",
    "\n",
    "        self.time_log = 0 # í•™ìŠµ íš¨ìœ¨ì„ ì‹œê°„ìœ¼ë¡œ Plot í•˜ê¸°ìœ„í•´ ë„ì…í•œ ë³€ìˆ˜\n",
    "                \n",
    "        # DQN hyperparameters\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "\n",
    "        self.state_dim = (self.env.observation_space.shape[0], )\n",
    "        try: self.action_dim = self.env.action_space.n # Discrete action\n",
    "        except: self.action_dim = env.action_space.shape[0] # Continous action \n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.update_cnt = 0 # q_behave ì—…ë°ì´íŠ¸ íšŸìˆ˜\n",
    "        self.update_freq = update_freq # q_behave ì—…ë°ì´íŠ¸ ì£¼ê¸°\n",
    "        self.update_target_freq = update_target_freq # q_target ì—…ë°ì´íŠ¸ ì£¼ê¸°\n",
    "        self.update_push_freq = update_push_freq # parameter serverì— ë³´ë‚´ëŠ” ì£¼ê¸°\n",
    "        self.device = device\n",
    "        self.total_steps = 0\n",
    "        self.scores = []\n",
    "        self.losses = [0]\n",
    "\n",
    "        self.q_behave = QNetwork(self.state_dim, self.action_dim, hidden).to(self.device)\n",
    "        self.q_target = QNetwork(self.state_dim, self.action_dim, hidden).to(self.device)\n",
    "        self.q_target.load_state_dict(self.q_behave.state_dict())\n",
    "        self.q_target.eval()\n",
    "        self.push_parameters()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_behave.parameters(), lr=learning_rate) \n",
    "\n",
    "    # ì €ì¥ëœ bufferì—ì„œ ë°ì´í„°ë¥¼ ë¡œë”©í•œ í›„ q_networkì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "    def update_q_network(self):\n",
    "        # update_cntë¥¼ q_behaveë¥¼ ì—…ë°ì´íŠ¸ í•  ë•Œë§ˆë‹¤ 1ì”© ìƒìŠ¹ (self.update_target_freq ë§Œí¼ q_behaveë¥¼ ì—…ë°ì´íŠ¸ë¥¼ í•  ë•Œë§ˆë‹¤ q_targetì„ ì—…ë°ì´íŠ¸ í•˜ê¸° ìœ„í•¨)\n",
    "        self.update_cnt += 1\n",
    "        batch = ray.get(self.memory.batch_load.remote(self.batch_size)) \n",
    "        loss = self._compute_loss(batch) \n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.losses.append(loss.item()) # for plotting the losses\n",
    "        self.memory.batch_update_on.remote()\n",
    "\n",
    "    def target_hard_update(self):  \n",
    "        # Hard update ë°©ì‹\n",
    "        self.q_target.load_state_dict(self.q_behave.state_dict()) \n",
    "\n",
    "    def eval_select_action(self, state): \n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0) \n",
    "        Qs = self.q_behave(state) \n",
    "        action = Qs.argmax() \n",
    "        return Qs.detach().cpu().numpy(), action.detach().item() \n",
    "\n",
    "    def push_parameters(self):\n",
    "        # Send paramters to server \n",
    "        copied_model = deepcopy(self.q_behave).cpu()\n",
    "        self.params_server.update_parameters.remote(copied_model.state_dict())\n",
    "\n",
    "    def train(self):\n",
    "        # ì—¬ê¸°ì„œëŠ” trainingì˜ ì¢…ë£Œì‹œì ì„ ì •í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
    "        print(\"training start..\")\n",
    "\n",
    "        # LearnerëŠ” environmentì™€ ìƒí˜¸ì‘ìš©ì„ í•  í•„ìš”ê°€ ì—†ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” learnerì˜ í•™ìŠµë¥ ë„ plotí•´ë³´ê¸° ìœ„í•´ì„œ ë„ì…\n",
    "        score = 0 \n",
    "        state = self.env.reset() \n",
    "        start_time = int(time.time())  \n",
    "        while 1: \n",
    "            Qs, action = self.select_action(state) \n",
    "            next_state, reward, done, _ = self.env.step(action)  \n",
    "            score += reward \n",
    "            state = next_state \n",
    "            self.epsilon = max(self.epsilon-self.eps_decay, self.eps_min) \n",
    "#             time.sleep(0.0002) # learnerì˜ í•™ìŠµ ì†ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤\n",
    "\n",
    "            self.update_q_network() \n",
    "            # ë§Œì¼ update_push_freq íšŸìˆ˜ ë§Œí¼ q_behaveë¥¼ ì—…ë°ì´íŠ¸ í–ˆë‹¤ë©´, serverì— parameterë¥¼ ë³´ëƒ…ë‹ˆë‹¤. \n",
    "            # í˜„ì¬ëŠ” target_networkì´ ì—…ë°ì´íŠ¸ ë ë•Œ, í•¨ê»˜ parameterì— ë³´ë‚´ê¸° ë•Œë¬¸ì— passë¡œ í–ˆìŠµë‹ˆë‹¤.\n",
    "            if (self.update_cnt%self.update_push_freq)==0: pass\n",
    "\n",
    "            # ë§Œì¼ target_update_freqì˜ íšŸìˆ˜ ë§Œí¼ q_behaveë¥¼ ì—…ë°ì´íŠ¸ í–ˆë‹¤ë©´, target_networkì„ ë³µì‚¬í•´ì˜µë‹ˆë‹¤.\n",
    "            if (self.update_cnt%self.update_target_freq)==0: \n",
    "                self.target_hard_update()\n",
    "                self.push_parameters()\n",
    "#             print(\"return_total_store_count\", ray.get(self.memory.return_total_store_count.remote()))\n",
    "            if done:\n",
    "                self.time_log = int(time.time()) - start_time # Accumulated Trainning Time (unit: second)\n",
    "                state = self.env.reset() \n",
    "                self.plot_util.store_learner_data.remote(score) \n",
    "                self._plot_status(score)\n",
    "                score = 0\n",
    "\n",
    "    def select_action(self, state): \n",
    "        if np.random.random() < self.epsilon: \n",
    "            return np.zeros(self.action_dim), self.env.action_space.sample() \n",
    "        else: \n",
    "            state = torch.FloatTensor(state).to(self.device).unsqueeze(0) \n",
    "            Qs = self.q_behave(state) \n",
    "            action = Qs.argmax() \n",
    "            return Qs.detach().cpu().numpy(), action.detach().item() \n",
    "\n",
    "    def _compute_loss(self, batch: \"Dictionary (S, A, R', S', Dones)\"):\n",
    "        states = torch.FloatTensor(batch['states']).to(self.device)\n",
    "        next_states = torch.FloatTensor(batch['next_states']).to(self.device)\n",
    "        actions = torch.LongTensor(batch['actions'].reshape(-1, 1)).to(self.device)\n",
    "        rewards = torch.FloatTensor(batch['rewards'].reshape(-1, 1)).to(self.device)\n",
    "        dones = torch.FloatTensor(batch['dones'].reshape(-1, 1)).to(self.device)\n",
    "        \n",
    "        current_q = self.q_behave(states).gather(1, actions)\n",
    "        next_q = self.q_target(next_states).max(dim=1, keepdim=True)[0].detach()\n",
    "        mask = 1 - dones\n",
    "        target = (rewards + (mask * self.gamma * next_q)).to(self.device)\n",
    "        loss = F.smooth_l1_loss(target, current_q)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _plot_status(self, score): \n",
    "        if self.plot_mode=='wandb': \n",
    "            wandb.log({\"Learner Score\": score, \n",
    "                       \"Learner Epsilon\": self.epsilon, \n",
    "                       \"loss(10 frames avg)\": np.mean(self.losses[-10:]), \n",
    "                       \"Number of frames\": ray.get(self.memory.return_total_store_count.remote()) \n",
    "                      }, step=self.time_log) \n",
    "            \n",
    "        elif self.plot_mode=='inline':\n",
    "            ''' Plotting in jupyter notebook '''\n",
    "            score_dict = ray.get(self.plot_util.get_status.remote()) \n",
    "            clear_output(True)\n",
    "            plt.figure(facecolor='w', figsize=(25,25)) \n",
    "            for idx in range(1, len(score_dict)):\n",
    "                i,j = (idx-1)//5, (idx-1)%5 \n",
    "                plt.subplot2grid((5,5), (i,j)) \n",
    "                plt.plot(score_dict[idx]) \n",
    "                plt.title(f\"Score of Actor {idx}\")\n",
    "            if (j+1)%5==0:\n",
    "                plt.subplot2grid((5,5), (i+1,0)) \n",
    "            else:\n",
    "                plt.subplot2grid((5,5), (i,j+1)) \n",
    "            plt.plot(score_dict['learner']) \n",
    "            plt.title(\"Score of Learner\") \n",
    "            plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB_GROUP_NAME 6417_Dist_DQN_Luna\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter ì„¤ì •\n",
    "env_lists = ['CartPole-v0', 'LunarLander-v2']\n",
    "num_actors = 8 # actorì˜ ê°œìˆ˜\n",
    "actor_device = \"cpu\"\n",
    "actor_update_freq = 300\n",
    "\n",
    "# # lunarlander\n",
    "env_name = env_lists[1]\n",
    "buffer_size = 10000 \n",
    "update_buf_start = 5000\n",
    "learning_rate = 0.003\n",
    "hidden = 256 \n",
    "epsilon = 1.\n",
    "gamma = 0.99\n",
    "batch_size = 256\n",
    "update_freq = 1\n",
    "update_target_freq = 200\n",
    "update_push_freq = 1\n",
    "eps_decay_list = np.linspace(1/80000, 1/60000, num_actors)\n",
    "eps_min_list = np.linspace(0.1, 0.03, num_actors)\n",
    "config_dict = { \n",
    "    \"learner_eps_decay\" : eps_decay_list[-1],\n",
    "    \"learner_eps_min\" : 0.0, # make the learner deterministic when reaching the minimum epsilon\n",
    "    \"eps_decay_list\" : eps_decay_list,\n",
    "    \"eps_min_list\" : eps_min_list\n",
    "} \n",
    "\n",
    "# # CartPole\n",
    "# env_name = env_lists[0]\n",
    "# buffer_size = 4000\n",
    "# update_buf_start = 100\n",
    "# learning_rate = 0.001\n",
    "# hidden = 128\n",
    "# epsilon = 1.\n",
    "# gamma = 0.99\n",
    "# batch_size = 32   \n",
    "# update_freq = 1\n",
    "# update_target_freq = 150\n",
    "# update_push_freq = 1\n",
    "# eps_decay_list = np.linspace(1/1000, 1/4000, num_actors)\n",
    "# eps_min_list = np.linspace(0.125, 0.02, num_actors)\n",
    "# config_dict = { \n",
    "#     \"learner_eps_decay\" : eps_decay_list[num_actors//2],\n",
    "#     \"learner_eps_min\" : 0.0, # make the learner deterministic when reaching the minimum epsilon\n",
    "#     \"eps_decay\" : eps_decay_list,\n",
    "#     \"eps_min\" : eps_min_list\n",
    "# } \n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_dim = (env.observation_space.shape[0], ) \n",
    "\n",
    "learner_device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "plot_mode = 'wandb' # plot options: 'wandb' or 'inline' or False\n",
    "WANDB_GROUP_NAME = str(np.random.randint(10000))+ '_Dist_DQN_' + env_name[:4]\n",
    "print(\"WANDB_GROUP_NAME\", WANDB_GROUP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/rl_flip_school_team/Distributed_DQN\" target=\"_blank\">https://app.wandb.ai/rl_flip_school_team/Distributed_DQN</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/rl_flip_school_team/Distributed_DQN/runs/1ygotfhd\" target=\"_blank\">https://app.wandb.ai/rl_flip_school_team/Distributed_DQN/runs/1ygotfhd</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "params_server = Network_parameter_server.remote() \n",
    "memory = ReplayBuffer.remote(buffer_size, state_dim)\n",
    "plot_util = Plot_inline.remote()\n",
    "learner_eps_decay = config_dict[\"learner_eps_decay\"]\n",
    "learner_eps_min = config_dict[\"learner_eps_min\"]\n",
    "\n",
    "WANDB_CONFIG_learner={\"env_name\": env_name, \n",
    "            \"gamma\": gamma,\n",
    "            \"num_actors\": num_actors,\n",
    "            \"buffer_size\": buffer_size,\n",
    "            \"update_start_buffer_size\": update_buf_start,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"hidden\": hidden,\n",
    "            \"target_update_freq (unit:step)\": update_target_freq,\n",
    "            \"behave_update_freq (unit:step)\": update_freq,\n",
    "            \"push_to_params_server_frew (unit:step)\": update_push_freq,\n",
    "            \"eps_max\": epsilon,\n",
    "            \"eps_min\": learner_eps_min,\n",
    "            \"eps_decay\": learner_eps_decay,\n",
    "            }\n",
    "learner = Learner(env_name, params_server, memory, gamma, epsilon, learner_eps_decay, learner_eps_min,\n",
    "                  update_freq, update_target_freq, update_push_freq, \n",
    "                  hidden, batch_size, learning_rate, learner_device, plot_mode, plot_util, WANDB_GROUP_NAME, WANDB_CONFIG_learner) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë™ì  ë³€ìˆ˜ í• ë‹¹ì„ í†µí•´ì„œ, actorë¥¼ ë‹¤ìˆ˜ì˜ ì„œë¡œ ë‹¤ë¥¸ indexë¡œ ë³€ìˆ˜ë¥¼ ì„ ì–¸í•©ë‹ˆë‹¤ (ë‹¤ë§Œ, rayëŠ” ì„œë¡œ ë‹¤ë¥¸ ë³€ìˆ˜ë¡œ actorë¥¼ ì§€ì •í•˜ì§€ ì•Šì•„ë„ ì˜ ì‘ë™í•©ë‹ˆë‹¤).\n",
    "# ë¨¼ì € replay bufferë¥¼ learnerê°€ updateí•  ìˆ˜ ìˆëŠ” ìƒíƒœê°€ ë˜ë„ë¡ ì±„ì›ë‹ˆë‹¤.\n",
    "# forë¬¸ ì•ˆì„ ë³´ì‹œë©´ globals()ë¡œ ì„ ì–¸ì´ ë˜ì–´ìˆëŠ”ë°, ì´ëŠ” ë°˜ë“œì‹œ í•„ìš”í•œ ì‘ì—…ì€ ì•„ë‹™ë‹ˆë‹¤. actorë¼ëŠ” ë‹¨ì¼ ë³€ìˆ˜ë¡œ ë°›ì•„ë„ ì‹¤í–‰ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "buffer_ready_done = [] \n",
    "for actor_idx in range(1, num_actors+1):\n",
    "\n",
    "    eps_min = config_dict[\"eps_min_list\"][actor_idx-1]\n",
    "    eps_decay = config_dict[\"eps_decay_list\"][actor_idx-1]\n",
    "    WANDB_CONFIG={\"env_name\": env_name, \n",
    "                \"actor_index\": actor_idx,\n",
    "                \"gamma\": gamma,\n",
    "                \"buffer_size\": buffer_size,\n",
    "                \"update_start_buffer_size\": update_buf_start,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"hidden\": hidden,\n",
    "                \"target_update_freq (unit:step)\": update_target_freq,\n",
    "                \"behave_update_freq (unit:step)\": update_freq,\n",
    "                \"push_to_params_server_frew (unit:step)\": update_push_freq,\n",
    "                \"eps_max\": epsilon,\n",
    "                \"eps_min\": eps_min,\n",
    "                \"eps_decay\": eps_decay,\n",
    "                }\n",
    "    globals()[f\"actor_{actor_idx}\"] = Actor.remote(params_server, memory, env_name, actor_idx, actor_update_freq, update_buf_start,\n",
    "                                                   epsilon, eps_decay, eps_min, hidden, actor_device, plot_mode, plot_util, num_actors, WANDB_GROUP_NAME, WANDB_CONFIG)\n",
    "    buffer_ready_done.append(globals()[f\"actor_{actor_idx}\"].make_buffer_ready.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=60057)\u001b[0m wandb: Tracking run with wandb version 0.9.4\n",
      "\u001b[2m\u001b[36m(pid=60061)\u001b[0m wandb: Tracking run with wandb version 0.9.4\n",
      "\u001b[2m\u001b[36m(pid=60006)\u001b[0m wandb: Tracking run with wandb version 0.9.4\n",
      "\u001b[2m\u001b[36m(pid=60016)\u001b[0m wandb: Tracking run with wandb version 0.9.4\n",
      "\u001b[2m\u001b[36m(pid=60014)\u001b[0m wandb: Tracking run with wandb version 0.9.4\n",
      "\u001b[2m\u001b[36m(pid=60015)\u001b[0m wandb: Tracking run with wandb version 0.9.4\n",
      "\u001b[2m\u001b[36m(pid=60058)\u001b[0m wandb: Tracking run with wandb version 0.9.4\n",
      "\u001b[2m\u001b[36m(pid=60059)\u001b[0m wandb: Tracking run with wandb version 0.9.4\n",
      "\u001b[2m\u001b[36m(pid=60057)\u001b[0m wandb: Wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(pid=60057)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=60057)\u001b[0m wandb: Run data is saved locally in wandb/run-20210222_112741-1eswgmlq\n",
      "\u001b[2m\u001b[36m(pid=60061)\u001b[0m wandb: Wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(pid=60061)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=60061)\u001b[0m wandb: Run data is saved locally in wandb/run-20210222_112741-y4gr1c4r\n",
      "\u001b[2m\u001b[36m(pid=60016)\u001b[0m wandb: Wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(pid=60016)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=60016)\u001b[0m wandb: Run data is saved locally in wandb/run-20210222_112741-37asc9tg\n",
      "\u001b[2m\u001b[36m(pid=60006)\u001b[0m wandb: Wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(pid=60006)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=60006)\u001b[0m wandb: Run data is saved locally in wandb/run-20210222_112741-z0js2ks9\n",
      "\u001b[2m\u001b[36m(pid=60014)\u001b[0m wandb: Wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(pid=60014)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=60014)\u001b[0m wandb: Run data is saved locally in wandb/run-20210222_112741-t67kigz6\n",
      "\u001b[2m\u001b[36m(pid=60058)\u001b[0m wandb: Wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(pid=60058)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=60058)\u001b[0m wandb: Run data is saved locally in wandb/run-20210222_112741-2wva9axe\n",
      "\u001b[2m\u001b[36m(pid=60059)\u001b[0m wandb: Wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(pid=60059)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=60059)\u001b[0m wandb: Run data is saved locally in wandb/run-20210222_112741-15ypp2r3\n",
      "\u001b[2m\u001b[36m(pid=60015)\u001b[0m wandb: Wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(pid=60015)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=60015)\u001b[0m wandb: Run data is saved locally in wandb/run-20210222_112741-m5n7uji5\n",
      "\u001b[2m\u001b[36m(pid=60061)\u001b[0m wandb: Syncing run 4_Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60061)\u001b[0m wandb: â­ï¸ View project at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60061)\u001b[0m wandb: ğŸš€ View run at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN/runs/y4gr1c4r\n",
      "\u001b[2m\u001b[36m(pid=60061)\u001b[0m wandb: Run `wandb off` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=60061)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=60057)\u001b[0m wandb: Syncing run 5_Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60057)\u001b[0m wandb: â­ï¸ View project at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60057)\u001b[0m wandb: ğŸš€ View run at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN/runs/1eswgmlq\n",
      "\u001b[2m\u001b[36m(pid=60057)\u001b[0m wandb: Run `wandb off` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=60057)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=60016)\u001b[0m wandb: Syncing run 1_Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60016)\u001b[0m wandb: â­ï¸ View project at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60016)\u001b[0m wandb: ğŸš€ View run at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN/runs/37asc9tg\n",
      "\u001b[2m\u001b[36m(pid=60016)\u001b[0m wandb: Run `wandb off` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=60016)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=60014)\u001b[0m wandb: Syncing run 2_Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60014)\u001b[0m wandb: â­ï¸ View project at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60014)\u001b[0m wandb: ğŸš€ View run at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN/runs/t67kigz6\n",
      "\u001b[2m\u001b[36m(pid=60014)\u001b[0m wandb: Run `wandb off` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=60014)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=60006)\u001b[0m wandb: Syncing run 8_Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60006)\u001b[0m wandb: â­ï¸ View project at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60006)\u001b[0m wandb: ğŸš€ View run at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN/runs/z0js2ks9\n",
      "\u001b[2m\u001b[36m(pid=60006)\u001b[0m wandb: Run `wandb off` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=60006)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=60058)\u001b[0m wandb: Syncing run 6_Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60058)\u001b[0m wandb: â­ï¸ View project at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60058)\u001b[0m wandb: ğŸš€ View run at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN/runs/2wva9axe\n",
      "\u001b[2m\u001b[36m(pid=60058)\u001b[0m wandb: Run `wandb off` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=60058)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=60059)\u001b[0m wandb: Syncing run 3_Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60059)\u001b[0m wandb: â­ï¸ View project at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60059)\u001b[0m wandb: ğŸš€ View run at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN/runs/15ypp2r3\n",
      "\u001b[2m\u001b[36m(pid=60059)\u001b[0m wandb: Run `wandb off` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=60059)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=60015)\u001b[0m wandb: Syncing run 7_Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60015)\u001b[0m wandb: â­ï¸ View project at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN\n",
      "\u001b[2m\u001b[36m(pid=60015)\u001b[0m wandb: ğŸš€ View run at https://app.wandb.ai/rl_flip_school_team/Distributed_DQN/runs/m5n7uji5\n",
      "\u001b[2m\u001b[36m(pid=60015)\u001b[0m wandb: Run `wandb off` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=60015)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=60057)\u001b[0m wandb: ERROR Error while calling W&B API: Error 1213: Deadlock found when trying to get lock; try restarting transaction (<Response [500]>)\n",
      "\u001b[2m\u001b[36m(pid=60057)\u001b[0m wandb: Network error resolved after 0:00:01.712950, resuming normal operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Done in Actor-#1', 'Done in Actor-#2', 'Done in Actor-#3', 'Done in Actor-#4', 'Done in Actor-#5', 'Done in Actor-#6', 'Done in Actor-#7', 'Done in Actor-#8']\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë“  actorì˜ í™œë™ì´ ì¢…ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "print(ray.get(buffer_ready_done)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì‹œì‘\n",
    "for actor_idx in range(1, num_actors+1):\n",
    "    globals()[f\"actor_{actor_idx}\"].explore.remote() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonic/.conda/envs/RL_Env/lib/python3.7/site-packages/ipykernel_launcher.py:144: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "learner.train()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
